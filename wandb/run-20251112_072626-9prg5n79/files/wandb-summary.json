{"episode_goals":0,"Value Function Update Magnitude":0.21161696314811707,"Timesteps Collected":50012,"Value Function Loss":0.031473301661511265,"Collected Steps per Second":10249.710328482002,"Overall Steps per Second":7226.118319569646,"Timestep Consumption Time":2.0416474528610706,"x_vel":7.788571106263922,"z_vel":-25.824096720329763,"Timestep Collection Time":4.879357405938208,"total_touches":0,"Policy Update Magnitude":0.105771504342556,"_timestamp":1.7629363217637944e+09,"total_goals":0,"PPO Batch Consumption Time":0.02762905756632487,"Cumulative Model Updates":46668,"episode_touches":0,"Policy Entropy":1.1566709280014038,"y_vel":137.88211133223,"Cumulative Timesteps":389590518,"SB3 Clip Fraction":0.12810332824786505,"_step":15722,"_wandb":{"runtime":85526},"Mean KL Divergence":0.02071653725579381,"Policy Reward":137.65239187738317,"Total Iteration Time":6.921004858799279,"_runtime":85526}